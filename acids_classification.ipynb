{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Details\n",
    "**Objective**: Create a new residue contact prediction model that leverages both the ESM2 embeddings of the\n",
    "input sequence residues and structural data from similar sequences. \\n\n",
    "**Implementation**:\n",
    "* Implement your model in Python, modifying or extending the ESM2 model to incorporate\n",
    "additional structural information. The input of the model is a single sequence, the output is a binary\n",
    "contact map (1 - if there is a contact between the residue pair, 0 - if there is no contact).\n",
    "* Use an open-source protein database, such as the Protein Data Bank (PDB), to access sequences\n",
    "and their 3D structural data for training and evaluation.\n",
    "* Ensure that the dataset is split into training, validation, and test sets, with a proper handling of\n",
    "sequence similarity to avoid train-test data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import EsmTokenizer, EsmModel\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "from prody import parsePDB, buildDistMatrix\n",
    "from Bio.PDB.Polypeptide import three_to_one\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import joblib\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use esm2_t6_8M_UR50D model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t6_8M_UR50D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be specify the range of proteins. We will pick in cell proteins in human, asymetric, with alpha and beta structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9090"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with (open(\"rcsb_pdb_ids_cell_HS_mem_asym_ab.txt\") as f):\n",
    "    pdb_ids = f.read()\n",
    "\n",
    "pdb_ids = pdb_ids.split(',')\n",
    "len(pdb_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to technical limitations let's test training pipeline on 20 proteins sample. With following filtering out proteins with mor than 10000 atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "pdb_ids = np.random.choice(pdb_ids,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdb_files(pdb_ids, download_dir='pdb_structures'):\n",
    "    \"\"\"\n",
    "    Downloads PDB structures given a list of PDB IDs using the requests library.\n",
    "    \"\"\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    for pdb_id in pdb_ids:\n",
    "        entry_id = pdb_id.split('_')[0]\n",
    "        file_path = os.path.join(download_dir, f'{entry_id}.pdb')\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "        url = f'https://files.rcsb.org/download/{entry_id}.pdb'\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            file_path = os.path.join(download_dir, f'{entry_id}.pdb')\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Downloaded {entry_id}.pdb\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download {entry_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdb_files(pdb_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load sequences and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sequence_and_contacts(pdb_file):\n",
    "    \"\"\"\n",
    "    Extracts the amino acid sequence and computes the contact map from a PDB file using ProDy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        structure = parsePDB(pdb_file)\n",
    "        ca_atoms = structure.select('name CA')\n",
    "\n",
    "        if ca_atoms is None:\n",
    "            print(f\"No CA atoms found in {pdb_file}\")\n",
    "            return None, None\n",
    "\n",
    "        if structure.numAtoms() > 10000:\n",
    "            return None, None\n",
    "\n",
    "        # Extract sequence\n",
    "        residues = ca_atoms.getResnames()\n",
    "        seq = ''\n",
    "        for resname in residues:\n",
    "            try:\n",
    "                one_letter = three_to_one(resname)\n",
    "                seq += one_letter\n",
    "            except KeyError:\n",
    "                seq += 'X'  # Unknown amino acid\n",
    "\n",
    "        # Compute contact map\n",
    "        coords = ca_atoms.getCoords()\n",
    "        num_residues = len(coords)\n",
    "        contact_map = np.zeros((num_residues, num_residues), dtype=np.float32)\n",
    "\n",
    "        dist_matrix = buildDistMatrix(coords, coords)\n",
    "        contact_map = (dist_matrix < 8.0).astype(np.float32)  # Contact threshold\n",
    "\n",
    "        return seq, contact_map\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdb_file}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "@> 4369 atoms and 1 coordinate set(s) were parsed in 0.03s.\n",
      "@> 5396 atoms and 1 coordinate set(s) were parsed in 0.03s.\n",
      "@> 2388 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> 5577 atoms and 1 coordinate set(s) were parsed in 0.03s.\n",
      "@> 5871 atoms and 1 coordinate set(s) were parsed in 0.03s.\n",
      "@> 2054 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> 4979 atoms and 1 coordinate set(s) were parsed in 0.02s.\n",
      "@> 1007 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> 2865 atoms and 1 coordinate set(s) were parsed in 0.02s.\n",
      "@> 11859 atoms and 1 coordinate set(s) were parsed in 0.07s.\n",
      "@> 2764 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> 1039 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> 2552 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> 2141 atoms and 1 coordinate set(s) were parsed in 0.01s.\n",
      "@> 5677 atoms and 1 coordinate set(s) were parsed in 0.03s.\n",
      "@> 12818 atoms and 1 coordinate set(s) were parsed in 0.07s.\n",
      "@> 11675 atoms and 1 coordinate set(s) were parsed in 0.07s.\n",
      "@> 5916 atoms and 1 coordinate set(s) were parsed in 0.03s.\n",
      "@> 2575 atoms and 1 coordinate set(s) were parsed in 0.02s.\n",
      "@> 1221 atoms and 1 coordinate set(s) were parsed in 0.01s.\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "contact_maps = []\n",
    "pdb_files = [os.path.join('pdb_structures', f'{pdb_id.split(\"_\")[0]}.pdb') for pdb_id in pdb_ids]\n",
    "\n",
    "# Check if files exist\n",
    "existing_pdb_files = []\n",
    "for pdb_file in pdb_files:\n",
    "    if os.path.exists(pdb_file):\n",
    "        existing_pdb_files.append(pdb_file)\n",
    "    else:\n",
    "        print(f\"File not found: {pdb_file}\")\n",
    "\n",
    "for pdb_file in existing_pdb_files:\n",
    "    seq, cmap = extract_sequence_and_contacts(pdb_file)\n",
    "    if seq is not None and cmap is not None:\n",
    "        sequences.append(seq)\n",
    "        contact_maps.append(cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's split data and make embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_esm2_embeddings(sequences):\n",
    "    \"\"\"\n",
    "    Obtains ESM2 embeddings for a list of sequences using Hugging Face Transformers.\n",
    "    \"\"\"\n",
    "    # Load the ESM2 model and tokenizer from the Hugging Face Hub\n",
    "    model_name = model_checkpoint\n",
    "    tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "    model = EsmModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for seq in tqdm(sequences, desc=\"Computing ESM2 embeddings\"):\n",
    "        # Tokenize the sequence\n",
    "        inputs = tokenizer(seq, return_tensors='pt', add_special_tokens=False)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Extract the embeddings from the last hidden state\n",
    "            token_embeddings = outputs.last_hidden_state  # (1, sequence_length, hidden_size)\n",
    "            embedding = token_embeddings[0, :, :]  # (sequence_length, hidden_size)\n",
    "            embeddings.append(embedding.numpy().astype(np.float32))  # Convert to float32\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 11\n",
      "Number of validation samples: 3\n",
      "Number of test samples: 3\n"
     ]
    }
   ],
   "source": [
    "# Simple data splitting without clustering\n",
    "np.random.seed(42)\n",
    "num_sequences = len(sequences)\n",
    "indices = list(range(num_sequences))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_end = int(0.7 * num_sequences)\n",
    "val_end = int(0.85 * num_sequences)\n",
    "\n",
    "train_indices = indices[:train_end]\n",
    "val_indices = indices[train_end:val_end]\n",
    "test_indices = indices[val_end:]\n",
    "\n",
    "train_sequences = [sequences[i] for i in train_indices]\n",
    "val_sequences = [sequences[i] for i in val_indices]\n",
    "test_sequences = [sequences[i] for i in test_indices]\n",
    "\n",
    "train_contact_maps = [contact_maps[i] for i in train_indices]\n",
    "val_contact_maps = [contact_maps[i] for i in val_indices]\n",
    "test_contact_maps = [contact_maps[i] for i in test_indices]\n",
    "\n",
    "# Verify dataset lengths\n",
    "print(f\"Number of training samples: {len(train_sequences)}\")\n",
    "print(f\"Number of validation samples: {len(val_sequences)}\")\n",
    "print(f\"Number of test samples: {len(test_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea717cce9e24ce3a3f8bebc48378c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing ESM2 embeddings:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeece4997d89473cb560cfab47672822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing ESM2 embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266489eb2ac04a46994047ebc3ecbf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing ESM2 embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get ESM2 embeddings\n",
    "train_embeddings = get_esm2_embeddings(train_sequences)\n",
    "val_embeddings = get_esm2_embeddings(val_sequences)\n",
    "test_embeddings = get_esm2_embeddings(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, contact_maps, esm_embeddings):\n",
    "        self.sequences = sequences\n",
    "        self.contact_maps = contact_maps\n",
    "        self.esm_embeddings = esm_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = self.esm_embeddings[idx]\n",
    "        contact_map = self.contact_maps[idx]\n",
    "\n",
    "        # Convert to tensors with float32 dtype\n",
    "        embedding = torch.tensor(embedding, dtype=torch.float32)\n",
    "        contact_map = torch.tensor(contact_map, dtype=torch.float32)\n",
    "\n",
    "        # Ensure dimensions match\n",
    "        L_emb = embedding.size(0)\n",
    "        L_contact = contact_map.size(0)\n",
    "        assert L_emb == L_contact, f\"Dimension mismatch: L_emb={L_emb}, L_contact={L_contact}\"\n",
    "\n",
    "        return embedding, contact_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ProteinDataset(train_sequences, train_contact_maps, train_embeddings)\n",
    "val_dataset = ProteinDataset(val_sequences, val_contact_maps, val_embeddings)\n",
    "test_dataset = ProteinDataset(test_sequences, test_contact_maps, test_embeddings)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_loader: 11\n",
      "Length of val_loader: 3\n",
      "Length of test_loader: 3\n"
     ]
    }
   ],
   "source": [
    "# Verify dataloader lengths\n",
    "print(f\"Length of train_loader: {len(train_loader)}\")\n",
    "print(f\"Length of val_loader: {len(val_loader)}\")\n",
    "print(f\"Length of test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactPredictionModel(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ContactPredictionModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # The input channels will be 2D (concatenated embeddings)\n",
    "        self.conv1 = nn.Conv2d(in_channels=embedding_dim * 2, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        \"\"\"\n",
    "        embedding: Tensor of shape (batch_size, L, D)\n",
    "        \"\"\"\n",
    "        batch_size, L, D = embedding.size()\n",
    "\n",
    "        # Compute pairwise features without using expand\n",
    "        emb_i = embedding.unsqueeze(2)  # (batch_size, L, 1, D)\n",
    "        emb_j = embedding.unsqueeze(1)  # (batch_size, 1, L, D)\n",
    "\n",
    "        # Use broadcasting to compute pairwise concatenation\n",
    "        emb_i = emb_i.repeat(1, 1, L, 1)  # (batch_size, L, L, D)\n",
    "        emb_j = emb_j.repeat(1, L, 1, 1)  # (batch_size, L, L, D)\n",
    "\n",
    "        pairwise_emb = torch.cat([emb_i, emb_j], dim=-1)  # (batch_size, L, L, 2D)\n",
    "\n",
    "        # Transpose to match Conv2d input: (batch_size, channels, height, width)\n",
    "        x = pairwise_emb.permute(0, 3, 1, 2).contiguous()  # (batch_size, 2D, L, L)\n",
    "\n",
    "        # Pass through convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = x.squeeze(1)  # Output shape: (batch_size, L, L)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for embedding, contact_map in tqdm(dataloader, desc=\"Training\"):\n",
    "        embedding = embedding.to(device)           # Shape: (batch_size, L, D)\n",
    "        contact_map = contact_map.to(device)       # Shape: (batch_size, L, L)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embedding)        # Shape: (batch_size, L, L)\n",
    "        loss = criterion(outputs, contact_map)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for embedding, contact_map in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            embedding = embedding.to(device)           # Shape: (batch_size, L, D)\n",
    "            contact_map = contact_map.to(device)       # Shape: (batch_size, L, L)\n",
    "\n",
    "            outputs = model(embedding)        # Shape: (batch_size, L, L)\n",
    "            loss = criterion(outputs, contact_map)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert outputs and targets to binary predictions\n",
    "            preds = (outputs >= 0.5).float()  # Threshold at 0.5\n",
    "            all_predictions.append(preds.cpu())\n",
    "            all_targets.append(contact_map.cpu())\n",
    "\n",
    "    # Concatenate all predictions and targets using reshape\n",
    "    all_predictions = torch.cat([p.reshape(-1) for p in all_predictions])\n",
    "    all_targets = torch.cat([t.reshape(-1) for t in all_targets])\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_targets.numpy(), all_predictions.numpy())\n",
    "    f1 = f1_score(all_targets.numpy(), all_predictions.numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, criterion, and optimizer\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Determine embedding dimension based on the ESM2 model output\n",
    "embedding_dim = train_embeddings[0].shape[-1]  # D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2a24b096814be0be1067ff19407e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c771f323b3dd410eae0d83189eb8d0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss = 0.5079, Val Loss = 0.3618, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385418181bea4706a7eb587855d09c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed86c79c29a1461982bdb5392f7f8425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Loss = 0.2235, Val Loss = 0.2526, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784564863cdd4d4da3299aca5f7dba0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3555fe6b07b0447d860512a113e22095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Train Loss = 0.1581, Val Loss = 0.2409, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4d59a5b08e4a56bc38d346c6442fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16d2831a7bd49f2aec1fcb777011012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Train Loss = 0.1500, Val Loss = 0.2454, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323454763b624460a488bcc349da4b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e088b2a5f25949739c1d6fef2cea0550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Train Loss = 0.1502, Val Loss = 0.2452, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6499e677f1374c48a018a39a5891077c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877d1d943f89493ba3f198c8a23f33c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Train Loss = 0.1487, Val Loss = 0.2429, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5118a1c70745e683d378db735ff5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59328445c5754f3a9134e63bae9c685f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Train Loss = 0.1474, Val Loss = 0.2406, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52df644244f7421b83bd0b91c1fed337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357026c705ce4a83b46c997fe170e140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Train Loss = 0.1468, Val Loss = 0.2398, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe94b662d194993bfe7feac342b5384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369879843e8a494c98e7bfd3e1e71d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: Train Loss = 0.1466, Val Loss = 0.2400, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1229d8dcacca4ed88e82c54e2d0f851f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f0574d0a7648bfb01a739eb1548403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: Train Loss = 0.1462, Val Loss = 0.2413, Val Accuracy = 0.9804, Val F1-score = 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67e539c90fa4a20a8ce90e4b0eaa7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 0.1171, Test Accuracy = 0.9807, Test F1-score = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = ContactPredictionModel(embedding_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy, val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, \"\n",
    "            f\"Val Accuracy = {val_accuracy:.4f}, Val F1-score = {val_f1:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_loss, test_accuracy, test_f1 = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss = {test_loss:.4f}, Test Accuracy = {test_accuracy:.4f}, Test F1-score = {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional structural information could be incorporated the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_structures(sequence, sequences, contact_maps, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Finds structural data from sequences similar to the input sequence.\n",
    "    \"\"\"\n",
    "    similar_structs = []\n",
    "    seq_len = len(sequence)\n",
    "    for seq, cmap in zip(sequences, contact_maps):\n",
    "        min_len = min(len(sequence), len(seq))\n",
    "        matches = sum(a == b for a, b in zip(sequence[:min_len], seq[:min_len]))\n",
    "        similarity = matches / min_len\n",
    "        if similarity > threshold:\n",
    "            # Resize contact map to match the length of the input sequence\n",
    "            cmap_resized = cmap[:seq_len, :seq_len]\n",
    "            similar_structs.append(cmap_resized)\n",
    "\n",
    "    # Aggregate structural information (e.g., mean contact map)\n",
    "    if similar_structs:\n",
    "        aggregated_struct = np.mean(similar_structs, axis=0).astype(np.float32)  # Ensure dtype is float32\n",
    "        # Ensure the aggregated structure has the correct dimensions\n",
    "        if aggregated_struct.shape[0] != seq_len:\n",
    "            aggregated_struct = aggregated_struct[:seq_len, :seq_len]\n",
    "    else:\n",
    "        # Return a zero matrix if no similar structures are found\n",
    "        aggregated_struct = np.zeros((seq_len, seq_len), dtype=np.float32)\n",
    "\n",
    "    return aggregated_struct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code uses only 20 sample due to technical limitations. There are several methods to overcome memory overload:\n",
    "* Switch to 16-Bit Precision\n",
    "* Use out-of-core processing with libraries like Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "formicidae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
